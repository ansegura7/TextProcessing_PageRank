Efficient_JJ URL_NN caching_NN for_IN world_NN wide_JJ web_NN crawling_VBG
Crawling_VBG the_DT web_NN is_VBZ deceptively_RB simple_JJ :_: the_DT basic_JJ algorithm_NN is_VBZ -LRB-_-LRB- a_DT -RRB-_-RRB- Fetch_VB a_DT page_NN -LRB-_-LRB- b_NN -RRB-_-RRB- Parse_VBP it_PRP to_TO extract_VB all_DT linked_VBN URLs_NNS -LRB-_-LRB- c_NN -RRB-_-RRB- For_IN all_PDT the_DT URLs_NNS not_RB seen_VBN before_RB ,_, repeat_NN -LRB-_-LRB- a_DT -RRB-_-RRB- -_: -LRB-_-LRB- c_NN -RRB-_-RRB- ._.
However_RB ,_, the_DT size_NN of_IN the_DT web_NN -LRB-_-LRB- estimated_VBN at_IN over_IN 4_CD billion_CD pages_NNS -RRB-_-RRB- and_CC its_PRP$ rate_NN of_IN change_NN -LRB-_-LRB- estimated_VBN at_IN 7_CD %_NN per_IN week_NN -RRB-_-RRB- move_VBP this_DT plan_NN from_IN a_DT trivial_JJ programming_NN exercise_NN to_TO a_DT serious_JJ algorithmic_JJ and_CC system_NN design_NN challenge_NN ._.
Indeed_RB ,_, these_DT two_CD factors_NNS alone_RB imply_VBP that_IN for_IN a_DT reasonably_RB fresh_JJ and_CC complete_JJ crawl_NN of_IN the_DT web_NN ,_, step_NN -LRB-_-LRB- a_DT -RRB-_-RRB- must_MD be_VB executed_VBN about_IN a_DT thousand_CD times_NNS per_IN second_NN ,_, and_CC thus_RB the_DT membership_NN test_NN -LRB-_-LRB- c_NN -RRB-_-RRB- must_MD be_VB done_VBN well_RB over_IN ten_CD thousand_CD times_NNS per_IN second_NN against_IN a_DT set_NN too_RB large_JJ to_TO store_NN in_IN main_JJ memory_NN ._.
This_DT requires_VBZ a_DT distributed_VBN architecture_NN ,_, which_WDT further_RB complicates_VBZ the_DT membership_NN test_NN ._.
A_DT crucial_JJ way_NN to_TO speed_VB up_RP the_DT test_NN is_VBZ to_TO cache_NN ,_, that_DT is_VBZ ,_, to_TO store_VB in_IN main_JJ memory_NN a_DT -LRB-_-LRB- dynamic_JJ -RRB-_-RRB- subset_NN of_IN the_DT ``_`` seen_VBN ''_'' URLs_NNS ._.
The_DT main_JJ goal_NN of_IN this_DT paper_NN is_VBZ to_TO carefully_RB investigate_VB several_JJ URL_NN caching_NN techniques_NNS for_IN web_NN crawling_NN ._.
We_PRP consider_VBP both_DT practical_JJ algorithms_NNS :_: random_JJ replacement_NN ,_, static_JJ cache_NN ,_, LRU_NNP ,_, and_CC CLOCK_NN ,_, and_CC theoretical_JJ limits_NNS :_: clairvoyant_JJ caching_NN and_CC infinite_JJ cache_NN ._.
We_PRP performed_VBD about_IN 1,800_CD simulations_NNS using_VBG these_DT algorithms_NNS with_IN various_JJ cache_NN sizes_NNS ,_, using_VBG actual_JJ log_NN data_NNS extracted_VBN from_IN a_DT massive_JJ 33_CD day_NN web_NN crawl_NN that_WDT issued_VBD over_IN one_CD billion_CD HTTP_NNP requests_NNS ._.
Our_PRP$ main_JJ conclusion_NN is_VBZ that_DT caching_NN is_VBZ very_RB effective_JJ -_: in_IN our_PRP$ setup_NN ,_, a_DT cache_NN of_IN roughly_RB 50,000_CD entries_NNS can_MD achieve_VB a_DT hit_NN rate_NN of_IN almost_RB 80_CD %_NN ._.
Interestingly_RB ,_, this_DT cache_NN size_NN falls_VBZ at_IN a_DT critical_JJ point_NN :_: a_DT substantially_RB smaller_JJR cache_NN is_VBZ much_RB less_RBR effective_JJ while_IN a_DT substantially_RB larger_JJR cache_NN brings_VBZ little_JJ additional_JJ benefit_NN ._.
We_PRP conjecture_NN that_IN such_JJ critical_JJ points_NNS are_VBP inherent_JJ to_TO our_PRP$ problem_NN and_CC venture_NN an_DT explanation_NN for_IN this_DT phenomenon_NN ._.
