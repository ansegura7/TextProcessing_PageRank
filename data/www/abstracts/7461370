IRLbot_NN :_: scaling_VBG to_TO 6_CD billion_CD pages_NNS and_CC beyond_IN
This_DT paper_NN shares_VBZ our_PRP$ experience_NN in_IN designing_VBG a_DT web_NN crawler_NN that_WDT can_MD download_VB billions_NNS of_IN pages_NNS using_VBG a_DT single-server_JJ implementation_NN and_CC models_NNS its_PRP$ performance_NN ._.
We_PRP show_VBP that_IN with_IN the_DT quadratically_RB increasing_VBG complexity_NN of_IN verifying_VBG URL_NN uniqueness_NN ,_, BFS_NNS crawl_VBP order_NN ,_, and_CC fixed_VBN per-host_JJ rate-limiting_JJ ,_, current_JJ crawling_VBG algorithms_NNS can_MD not_RB effectively_RB cope_VB with_IN the_DT sheer_JJ volume_NN of_IN URLs_NNS generated_VBN in_IN large_JJ crawls_VBZ ,_, highly-branching_JJ spam_NN ,_, legitimate_JJ multi-million-page_JJ blog_NN sites_NNS ,_, and_CC infinite_JJ loops_NNS created_VBN by_IN server-side_JJ scripts_NNS ._.
We_PRP offer_VBP a_DT set_NN of_IN techniques_NNS for_IN dealing_VBG with_IN these_DT issues_NNS and_CC test_VB their_PRP$ performance_NN in_IN an_DT implementation_NN we_PRP call_VBP IRLbot_NN ._.
In_IN our_PRP$ recent_JJ experiment_NN that_WDT lasted_VBD 41_CD days_NNS ,_, IRLbot_NN running_VBG on_IN a_DT single_JJ server_NN successfully_RB crawled_VBD 6.3_CD billion_CD valid_JJ HTML_NNP pages_NNS -LRB-_-LRB- $_$ 7.6_CD $_$ billion_CD connection_NN requests_NNS -RRB-_-RRB- and_CC sustained_VBD an_DT average_JJ download_NN rate_NN of_IN 319_CD mb\/s_NNS -LRB-_-LRB- 1,789_CD pages\/s_NN -RRB-_-RRB- ._.
Unlike_IN our_PRP$ prior_JJ experiments_NNS with_IN algorithms_NNS proposed_VBN in_IN related_JJ work_NN ,_, this_DT version_NN of_IN IRLbot_NN did_VBD not_RB experience_VB any_DT bottlenecks_NNS and_CC successfully_RB handled_VBD content_NN from_IN over_IN 117_CD million_CD hosts_NNS ,_, parsed_VBN out_RP 394_CD billion_CD links_NNS ,_, and_CC discovered_VBD a_DT subset_NN of_IN the_DT web_NN graph_NN with_IN 41_CD billion_CD unique_JJ nodes_NNS ._.
